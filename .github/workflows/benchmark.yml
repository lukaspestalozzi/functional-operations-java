name: Performance Benchmarks

on:
  pull_request:
    branches: [ main, develop ]
    types: [ opened, synchronize, reopened ]
  workflow_dispatch:
    inputs:
      ref:
        description: 'Branch or commit to benchmark'
        required: false
        default: 'main'

permissions:
  contents: read
  pull-requests: write

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest

    steps:
      - name: Checkout PR code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha || github.event.inputs.ref || github.ref }}

      - name: Set up JDK 17
        uses: actions/setup-java@v4
        with:
          java-version: '17'
          distribution: 'temurin'
          cache: 'maven'

      - name: Run performance benchmarks (quick mode)
        run: ./run-benchmarks.sh --quick

      - name: Run memory benchmarks with GC profiler
        run: ./run-benchmarks.sh --quick --gc "MemoryAllocation.*"

      - name: Display benchmark summary
        if: always()
        run: |
          echo "## ğŸš€ Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f target/jmh-result.json ]; then
            echo "### Throughput Comparison (ops/Î¼s - higher is better)" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "| Operation | ListOps | Streams | Speedup |" >> $GITHUB_STEP_SUMMARY
            echo "|-----------|---------|---------|---------|" >> $GITHUB_STEP_SUMMARY

            # Parse JSON and create comparison table
            python3 << 'PYTHON_SCRIPT' >> $GITHUB_STEP_SUMMARY
          import json
          import sys

          try:
              with open('target/jmh-result.json', 'r') as f:
                  results = json.load(f)

              # Group by operation and mode
              throughput = {}
              for r in results:
                  name = r['benchmark'].split('.')[-1]
                  mode = r.get('mode', 'thrpt')
                  if mode != 'thrpt':
                      continue

                  # Get score for listSize=1000 if available, otherwise first result
                  score = r['primaryMetric']['score']
                  params = r.get('params', {})
                  list_size = params.get('listSize', 'N/A')

                  if list_size == '1000' or name not in throughput:
                      throughput[name] = score

              # Match listOps vs streams
              ops = ['map', 'filter', 'chainedMapFilter', 'chainedFilterMap', 'complexChain']
              for op in ops:
                  listops_key = f'listOps_{op}'
                  streams_key = f'streams_{op}'

                  if listops_key in throughput and streams_key in throughput:
                      lo = throughput[listops_key]
                      st = throughput[streams_key]
                      speedup = lo / st if st > 0 else 0
                      emoji = "ğŸŸ¢" if speedup >= 1.5 else "ğŸŸ¡" if speedup >= 1.0 else "ğŸ”´"
                      print(f"| {op} | {lo:.3f} | {st:.3f} | {emoji} {speedup:.2f}x |")

          except Exception as e:
              print(f"| Error | - | - | {e} |")
          PYTHON_SCRIPT

            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Legend" >> $GITHUB_STEP_SUMMARY
            echo "- ğŸŸ¢ **1.5x+ faster** - Significant improvement" >> $GITHUB_STEP_SUMMARY
            echo "- ğŸŸ¡ **1.0-1.5x** - Moderate improvement" >> $GITHUB_STEP_SUMMARY
            echo "- ğŸ”´ **<1.0x** - Slower than Streams" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "---" >> $GITHUB_STEP_SUMMARY
            echo "*Run locally: \`./run-benchmarks.sh --quick\`*" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ Benchmark results not found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-pr-${{ github.event.pull_request.number || github.run_id }}
          path: |
            target/jmh-result.json
            target/jmh-memory-result.json
            target/*.log
          retention-days: 30

      - name: Comment PR with results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        continue-on-error: true
        with:
          script: |
            const fs = require('fs');

            let comment = '## ğŸš€ Performance Benchmark Results\n\n';

            if (fs.existsSync('target/jmh-result.json')) {
              try {
                const results = JSON.parse(fs.readFileSync('target/jmh-result.json', 'utf8'));

                // Group throughput results
                const throughput = {};
                for (const r of results) {
                  const name = r.benchmark.split('.').pop();
                  if (r.mode !== 'thrpt') continue;
                  const listSize = r.params?.listSize || 'N/A';
                  if (listSize === '1000' || !throughput[name]) {
                    throughput[name] = r.primaryMetric.score;
                  }
                }

                comment += '### Throughput Comparison (ops/Î¼s)\n\n';
                comment += '| Operation | ListOps | Streams | Speedup |\n';
                comment += '|-----------|---------|---------|--------:|\n';

                const ops = ['map', 'filter', 'chainedMapFilter', 'chainedFilterMap', 'complexChain'];
                for (const op of ops) {
                  const lo = throughput[`listOps_${op}`];
                  const st = throughput[`streams_${op}`];
                  if (lo && st) {
                    const speedup = lo / st;
                    const emoji = speedup >= 1.5 ? 'ğŸŸ¢' : speedup >= 1.0 ? 'ğŸŸ¡' : 'ğŸ”´';
                    comment += `| ${op} | ${lo.toFixed(3)} | ${st.toFixed(3)} | ${emoji} ${speedup.toFixed(2)}x |\n`;
                  }
                }

                comment += '\n**Legend:** ğŸŸ¢ 1.5x+ faster | ğŸŸ¡ 1.0-1.5x | ğŸ”´ slower\n\n';
                comment += '---\n';
                comment += '*Run locally: `./run-benchmarks.sh --quick`*';
              } catch (e) {
                comment += 'âœ… Benchmarks completed. See artifacts for details.\n';
              }
            } else {
              comment += 'âš ï¸ Benchmarks encountered an issue.\n\n';
              comment += 'Please check the workflow logs for details.';
            }

            try {
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              core.warning(`Could not create PR comment: ${error.message}`);
            }
